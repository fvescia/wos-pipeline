{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package('boto3', 'https://pypi.org/simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import boto3\n",
    "from concurrent.futures import ThreadPoolExecutor \n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- purchaseOrder: struct (nullable = true)\n",
      " |    |-- purchaseOrder@orderDate: timestamp (nullable = true)\n",
      " |    |-- shipTo: struct (nullable = true)\n",
      " |    |    |-- shipTo@country: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- street: string (nullable = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- state: string (nullable = true)\n",
      " |    |    |-- zip: double (nullable = true)\n",
      " |    |-- billTo: struct (nullable = true)\n",
      " |    |    |-- billTo@country: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- street: string (nullable = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- state: string (nullable = true)\n",
      " |    |    |-- zip: double (nullable = true)\n",
      " |    |-- comment: string (nullable = true)\n",
      " |    |-- items: struct (nullable = true)\n",
      " |    |    |-- item: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- item@partNum: string (nullable = true)\n",
      " |    |    |    |    |-- productName: string (nullable = true)\n",
      " |    |    |    |    |-- quantity: long (nullable = true)\n",
      " |    |    |    |    |-- USPrice: double (nullable = true)\n",
      " |    |    |    |    |-- comment: string (nullable = true)\n",
      " |    |    |    |    |-- shipDate: timestamp (nullable = true)"
     ]
    }
   ],
   "source": [
    "data = spark.read.parquet('s3://wos-bucket/PurchaseOrder.xml.parquet')\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "par_files = ['s3://wos-bucket/PurchaseOrder.xml.parquet',\n",
    "             's3://wos-bucket/p2.parquet',\n",
    "             's3://wos-bucket/p3.parquet']\n",
    "\n",
    "# Define read function\n",
    "def from_s3(file):\n",
    "    return spark.read.parquet(file)\n",
    "    \n",
    "# Read test Parquet files from S3 in parallel\n",
    "with ThreadPoolExecutor(max_workers = 3) as executor:\n",
    "    results = executor.map(from_s3, par_files)\n",
    "    \n",
    "df1, df2, df3 = results\n",
    "\n",
    "# Combine files into a single dataframe\n",
    "# From https://stackoverflow.com/questions/37612622/spark-unionall-multiple-dataframes\n",
    "dfs = [df1, df2, df3]\n",
    "new_data = reduce(DataFrame.unionAll, dfs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
